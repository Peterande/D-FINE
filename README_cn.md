<!--# [D-FINE: Redefine Regression Task of DETRs as Fine-grained Distribution Refinement](https://arxiv.org/abs/xxxxxx) -->
<h2 align="center">
  D-FINE: Redefine Regression Task of DETRs as Fine&#8209;grained&nbsp;Distribution&nbsp;Refinement
</h2>

<p align="center">
    <!-- <a href="https://github.com/lyuwenyu/RT-DETR/blob/main/LICENSE">
        <img alt="license" src="https://img.shields.io/badge/LICENSE-Apache%202.0-blue">
    </a> -->
    <a href="https://github.com/Peterande/D-FINE/blob/master/LICENSE">
        <img alt="license" src="https://img.shields.io/github/license/Peterande/D-FINE">
    </a>
    <a href="https://github.com/Peterande/D-FINE/pulls">
        <img alt="prs" src="https://img.shields.io/github/issues-pr/Peterande/D-FINE">
    </a>
    <a href="https://github.com/Peterande/D-FINE/issues">
        <img alt="issues" src="https://img.shields.io/github/issues/Peterande/D-FINE?color=pink">
    </a>
    <a href="https://github.com/Peterande/D-FINE">
        <img alt="issues" src="https://img.shields.io/github/stars/Peterande/D-FINE">
    </a>
    <a href="https://arxiv.org/abs/xxx.xxx">
        <img alt="arXiv" src="https://img.shields.io/badge/arXiv-xxx.xxx-red">
    </a>
    <a href="mailto: pengyansong@mail.ustc.edu.cn">
        <img alt="emal" src="https://img.shields.io/badge/contact_me-email-yellow">
    </a>
</p>

<p align="center">
    ğŸ“„ This is the official implementation of the paper:
    <br>
    <a href="https://arxiv.org/abs/xxxxxx">D-FINE: Redefine Regression Task of DETRs as Fine-grained Distribution Refinement</a>
</p>


<p align="center">
Yansong Peng, Hebei Li, Peixi Wu, Yueyi Zhang, Xiaoyan Sun, and Feng Wu
</p>

<!-- <table><tr>
<td><img src=https://github.com/Peterande/storage/blob/main/latency.png border=0 width=333></td>
<td><img src=https://github.com/Peterande/storage/blob/main/params.png border=0 width=333></td>
<td><img src=https://github.com/Peterande/storage/blob/main/flops.png border=0 width=333></td>
</tr></table> -->

<table><tr>
<td><img src=https://github.com/Peterande/storage/blob/main/stats_padded.png border=0 width=1000></td>
</tr></table>


## ğŸš€ Updates
- [x] **\[2024.10.3\]** Release D-FINE series.
<!-- - ğŸ”œ **\[Next\]** Release D-FINE series pretrained on Objects365. -->

## ğŸ” æ¢ç´¢D-FINEèƒŒåçš„å…³é”®åˆ›æ–°
[English](README.md) | ç®€ä½“ä¸­æ–‡
<details open>
<summary> ç®€ä»‹ </summary>

## D-FINEï¼šé‡æ–°å®šä¹‰ç›®æ ‡æ£€æµ‹ä¸­çš„å›å½’ä»»åŠ¡

D-FINEé‡æ–°å®šä¹‰äº†åŸºäºDETRçš„ç›®æ ‡æ£€æµ‹å™¨ä¸­çš„å›å½’ä»»åŠ¡ã€‚

**ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„FDRæ–¹æ³•å°†æ£€æµ‹æ¡†çš„ç”Ÿæˆè¿‡ç¨‹åˆ†è§£ä¸ºä¸¤ä¸ªå…³é”®æ­¥éª¤ï¼š**

1. **åˆå§‹æ¡†é¢„æµ‹**ï¼šä¸ä¼ ç»Ÿæ–¹æ³•ç±»ä¼¼ï¼Œé¦–å…ˆç”Ÿæˆåˆå§‹è¾¹ç•Œæ¡†ã€‚
2. **ç²¾ç»†åˆ†å¸ƒä¼˜åŒ–**ï¼šæ¨¡å‹è§£ç å±‚è¿­ä»£åœ°å¯¹å››ç»„æ¦‚ç‡åˆ†å¸ƒå‡½æ•°è¿›è¡Œé€å±‚è¿­ä»£ä¼˜åŒ–ã€‚é€šè¿‡è¿™äº›åˆ†å¸ƒå¯¹åˆå§‹è¾¹ç•Œæ¡†åœ°ä¸Šä¸‹å·¦å³è¾¹ç¼˜è¿›è¡Œç»†å¾®è°ƒèŠ‚æˆ–è¾ƒå¤§è°ƒæ•´ã€‚

### FDRçš„ä¸»è¦ä¼˜åŠ¿ï¼š
1. **ç®€åŒ–çš„ç›‘ç£**ï¼šåœ¨ä¼˜åŒ–æœ€ç»ˆæ¡†çš„åŒæ—¶ï¼Œå¯ä»¥ç”¨æ ‡ç­¾å’Œé¢„æµ‹ç»“æœä¹‹é—´çš„æ®‹å·®ä½œä¸ºè¿™äº›æ¦‚ç‡åˆ†å¸ƒå‡½æ•°çš„ä¼˜åŒ–ç›®æ ‡ã€‚è¿™ä½¿æ¯ä¸ªè§£ç å±‚èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°é›†ä¸­è§£å†³å…¶å½“å‰é¢ä¸´çš„ç‰¹å®šå®šä½è¯¯å·®ï¼Œéšç€å±‚æ•°åŠ æ·±ï¼Œå…¶ç›‘ç£ä¹Ÿå˜å¾—è¶Šæ¥è¶Šç®€å•ï¼Œä»è€Œç®€åŒ–äº†æ•´ä½“ä¼˜åŒ–è¿‡ç¨‹ã€‚

2. **å¤æ‚åœºæ™¯ä¸‹çš„é²æ£’æ€§**ï¼šè¿™äº›æ¦‚ç‡åˆ†å¸ƒæœ¬è´¨ä¸Šä»£è¡¨äº†å¯¹æ¯ä¸ªè¾¹ç•Œâ€œå¾®è°ƒâ€çš„è‡ªä¿¡ç¨‹åº¦ã€‚è¿™ä½¿ç³»ç»Ÿèƒ½å¤Ÿç‹¬ç«‹å»ºæ¨¡æ¯ä¸ªè¾¹ç•Œåœ¨å„ä¸ªé˜¶æ®µçš„ä¸ç¡®å®šæ€§ï¼Œä»è€Œåœ¨é®æŒ¡ã€è¿åŠ¨æ¨¡ç³Šå’Œä½å…‰ç…§ç­‰å¤æ‚çš„å®é™…åœºæ™¯ä¸‹è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ï¼Œç›¸æ¯”ç›´æ¥å›å½’å››ä¸ªå›ºå®šå€¼è¦æ›´ä¸ºç¨³å¥ã€‚

   
4. **çµæ´»çš„ä¼˜åŒ–æœºåˆ¶**ï¼šæ¦‚ç‡åˆ†å¸ƒé€šè¿‡åŠ æƒæ±‚å’Œè½¬åŒ–ä¸ºæœ€ç»ˆçš„è¾¹ç•Œæ¡†åç§»å€¼ã€‚ç²¾å¿ƒè®¾è®¡çš„åŠ æƒå‡½æ•°ç¡®ä¿åœ¨åˆå§‹æ¡†å‡†ç¡®æ—¶è¿›è¡Œç»†å¾®è°ƒæ•´ï¼Œè€Œåœ¨å¿…è¦æ—¶åˆ™æä¾›è¾ƒå¤§çš„ä¿®æ­£ã€‚

   
6. **ç ”ç©¶æ½œåŠ›ä¸å¯æ‰©å±•æ€§**ï¼šé€šè¿‡å°†å›å½’ä»»åŠ¡è½¬å˜ä¸ºç±»ä¼¼åˆ†ç±»ä»»åŠ¡çš„æ¦‚ç‡åˆ†å¸ƒé¢„æµ‹é—®é¢˜ï¼Œè¿™ä¸€æ¡†æ¶ä¸ä»…æé«˜äº†ä¸å…¶ä»–ä»»åŠ¡çš„å…¼å®¹æ€§ï¼Œå®ƒè¿˜ä½¿å¾—ç›®æ ‡æ£€æµ‹æ¨¡å‹å¯ä»¥å—ç›ŠäºçŸ¥è¯†è’¸é¦ã€å¤šä»»åŠ¡å­¦ä¹ å’Œåˆ†å¸ƒå»ºæ¨¡ç­‰æ›´å¤šé¢†åŸŸçš„åˆ›æ–°ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æ‰“å¼€äº†æ–°çš„å¤§é—¨ã€‚


<!-- æ’å…¥è§£é‡ŠFDRè¿‡ç¨‹çš„å›¾ -->
<p align="center">
    <img src="https://github.com/Peterande/storage/blob/main/fdr.png" alt="ç²¾ç»†åˆ†å¸ƒä¼˜åŒ–è¿‡ç¨‹" width="777">
</p>

## GO-LSDï¼šå°†FDRæ‰©å±•åˆ°çŸ¥è¯†è’¸é¦

GO-LSDï¼ˆå…¨å±€æœ€ä¼˜å®šä½è‡ªè’¸é¦ï¼‰åŸºäºFDRï¼Œé€šè¿‡åœ¨ç½‘ç»œå±‚é—´å®ç°å®šä½çŸ¥è¯†è’¸é¦ï¼Œè¿›ä¸€æ­¥æ‰©å±•äº†FDRçš„èƒ½åŠ›ã€‚éšç€FDRçš„å¼•å…¥ï¼Œå›å½’ä»»åŠ¡ç°åœ¨å˜æˆäº†æ¦‚ç‡åˆ†å¸ƒé¢„æµ‹ï¼Œè¿™å¸¦æ¥äº†ä¸¤ä¸ªä¸»è¦ä¼˜åŠ¿ï¼š

1. **çŸ¥è¯†ä¼ é€’**ï¼šæ¦‚ç‡åˆ†å¸ƒå¤©ç„¶æºå¸¦å®šä½çŸ¥è¯†ï¼Œå¯ä»¥é€šè¿‡è®¡ç®—KLDæŸå¤±ä»æ·±å±‚ä¼ é€’åˆ°æµ…å±‚ã€‚è¿™æ˜¯ä¼ ç»Ÿå›ºå®šæ¡†è¡¨ç¤ºï¼ˆç‹„æ‹‰å…‹Î´å‡½æ•°ï¼‰æ— æ³•å®ç°çš„ã€‚
   
3. **ä¸€è‡´çš„ä¼˜åŒ–ç›®æ ‡**ï¼šç”±äºæ¯ä¸€å±‚éƒ½å…±äº«ä¸€ä¸ªå…±åŒç›®æ ‡ï¼šå‡å°‘åˆå§‹è¾¹ç•Œæ¡†ä¸çœŸå®è¾¹ç•Œæ¡†ä¹‹é—´çš„æ®‹å·®ï¼›å› æ­¤æœ€åä¸€å±‚ç”Ÿæˆçš„ç²¾ç¡®æ¦‚ç‡åˆ†å¸ƒå¯ä»¥é€šè¿‡è’¸é¦å¼•å¯¼å‰å‡ å±‚ã€‚è¿™äº§ç”Ÿäº†ä¸€ç§åŒèµ¢çš„ååŒæ•ˆåº”ï¼šéšç€è®­ç»ƒçš„è¿›è¡Œï¼Œæœ€åä¸€å±‚çš„é¢„æµ‹å˜å¾—è¶Šæ¥è¶Šå‡†ç¡®ï¼Œå…¶ç”Ÿæˆçš„è½¯æ ‡ç­¾æ›´å¥½åœ°å¸®åŠ©å‰å‡ å±‚æé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚åè¿‡æ¥ï¼Œå‰å‡ å±‚å­¦ä¼šæ›´å¿«åœ°å®šä½åˆ°å‡†ç¡®ä½ç½®ï¼Œç®€åŒ–äº†æ·±å±‚çš„ä¼˜åŒ–ä»»åŠ¡ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ•´ä½“å‡†ç¡®æ€§ã€‚


<!-- æ’å…¥è§£é‡ŠGO-LSDè¿‡ç¨‹çš„å›¾ -->
<p align="center">
    <img src="https://github.com/Peterande/storage/blob/main/go_lsd.png" alt="GO-LSDè¿‡ç¨‹" width="777">
</p>

### D-FINEé¢„æµ‹çš„å¯è§†åŒ–

ä»¥ä¸‹å¯è§†åŒ–å±•ç¤ºäº†D-FINEåœ¨å„ç§å¤æ‚æ£€æµ‹åœºæ™¯ä¸­çš„é¢„æµ‹ç»“æœã€‚è¿™äº›åœºæ™¯åŒ…æ‹¬é®æŒ¡ã€ä½å…‰ç…§ã€è¿åŠ¨æ¨¡ç³Šã€æ™¯æ·±æ•ˆæœå’Œå¯†é›†åœºæ™¯ã€‚å°½ç®¡é¢å¯¹è¿™äº›æŒ‘æˆ˜ï¼ŒD-FINEä¾ç„¶èƒ½å¤Ÿäº§ç”Ÿå‡†ç¡®çš„å®šä½ç»“æœã€‚

<!-- æ’å…¥å¤æ‚åœºæ™¯ä¸­çš„é¢„æµ‹å¯è§†åŒ–å›¾ -->
<p align="center">
    <img src="https://github.com/Peterande/storage/blob/main/hard_case.png" alt="D-FINEåœ¨å¤æ‚åœºæ™¯ä¸­çš„é¢„æµ‹" width="777">
</p>

### FDRå’ŒGO-LSDä¼šå¸¦æ¥æ›´å¤šçš„æ¨ç†æˆæœ¬å—ï¼Ÿ
å¹¶ä¸ä¼šï¼ŒFDRå’ŒåŸå§‹çš„é¢„æµ‹å‡ ä¹æ²¡æœ‰åœ¨é€Ÿåº¦ã€å‚æ•°é‡å’Œè®¡ç®—å¤æ‚åº¦ä¸Šçš„ä»»ä½•åŒºåˆ«ï¼Œå®Œå…¨æ˜¯æ— æ„Ÿæ›¿æ¢ã€‚

### FDRå’ŒGO-LSDä¼šå¸¦æ¥æ›´å¤šçš„è®­ç»ƒæˆæœ¬å—ï¼Ÿ
è®­ç»ƒæˆæœ¬çš„å¢åŠ ä¸»è¦æ¥æºäºå¦‚ä½•ç”Ÿæˆåˆ†å¸ƒçš„æ ‡ç­¾ã€‚æˆ‘ä»¬å·²ç»å¯¹è¯¥è¿‡ç¨‹è¿›è¡Œäº†ä¼˜åŒ–ï¼Œå°†è®­ç»ƒæ—¶é•¿å’Œæ˜¾å­˜å ç”¨æ§åˆ¶åœ¨äº†6%å’Œ2%ï¼Œå‡ ä¹æ— æ„Ÿã€‚

</details>


## Model Zoo

### Base models
| Model | Dataset | AP<sup>val</sup> | #Params | FPS | GFLOPs | config | checkpoint | log |
| :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: | :---: |
**D-FINE-S** | COCO | **48.5** |  10M | 287 | 25 | [config](./configs/dfine/dfine_hgnetv2_s_coco.yml) | [48.5](https://github.com/Peterande/storage/releases/download/dfinev1/dfine_s_coco.pth) |
**D-FINE-M** | COCO | **52.3** |  19M | 180 | 57 | [config](./configs/dfine/dfine_hgnetv2_m_coco.yml) | [52.3](https://github.com/Peterande/storage/releases/download/dfinev1/dfine_m_coco.pth) |
**D-FINE-L** | COCO | **54.0** |  31M | 129 | 91 | [config](./configs/dfine/dfine_hgnetv2_l_coco.yml) | [54.0](https://github.com/Peterande/storage/releases/download/dfinev1/dfine_l_coco.pth) |
**D-FINE-X** | COCO | **55.8** |  62M | 81 | 202 | [config](./configs/dfine/dfine_hgnetv2_x_coco.yml) | [55.8](https://github.com/Peterande/storage/releases/download/dfinev1/dfine_x_coco.pth) |
**D-FINE-S** | COCO+Objects365 | **50.3** |  10M | 287 | 25 | [config](./configs/dfine/objects365/dfine_hgnetv2_s_obj2coco.yml) | []() |
**D-FINE-M** | COCO+Objects365 | **55.0** |  19M | 180 | 57 | [config](./configs/dfine/objects365/dfine_hgnetv2_m_obj2coco.yml) | []() |
**D-FINE-L** | COCO+Objects365 | **56.9** |  31M | 129 | 91 | [config](./configs/dfine/objects365/dfine_hgnetv2_l_obj2coco.yml) | []() |
**D-FINE-X** | COCO+Objects365 | **59.0** |  62M | 81 | 202 | [config](./configs/dfine/objects365/dfine_hgnetv2_x_obj2coco.yml) | []() |

**Notes:**
- `AP` is evaluated on *MSCOCO val2017* dataset.
- `FPS` is evaluated on a single T4 GPU with $batch\\_size = 1$, $fp16$, and $TensorRT==10.4.0$.
- `COCO+Objects365` in the table means finetuned model on `COCO` using pretrained weights trained on `Objects365`.
<!-- - `Stage 1`: AP<sup>val</sup> before tuning off advanced augmentations in the final few epochs (Objects365 AP<sup>val</sup> if dataset is `COCO+365`). \
These ckpts offering better generalization.
- `Stage 2`: Best AP<sup>val</sup> after disabling advanced augmentations in the final few epochs. (COCO AP<sup>val</sup> if dataset is `COCO+365`) -->

## Quick start

<details open>
<summary> Setup </summary>
  
```shell

pip install -r requirements.txt
```

</details>


<details>
  
<summary> COCO2017 dataset </summary>

1. Download COCO2017 from [OpenDataLab](https://opendatalab.com/OpenDataLab/COCO_2017). 
1. Modify paths in [coco_detection.yml](./configs/dataset/coco_detection.yml)

    ```yaml
    train_dataloader: 
        img_folder: /data/COCO2017/train2017/
        ann_file: /data/COCO2017/annotations/instances_train2017.json
    val_dataloader:
        img_folder: /data/COCO2017/val2017/
        ann_file: /data/COCO2017/annotations/instances_val2017.json
    ```
      
</details>

<details>
<summary> Objects365 dataset </summary>

1. Download Objects365 from [OpenDataLab](https://opendatalab.com/OpenDataLab/Objects365). 

2. Set the Base Directory:
```shell
export BASE_DIR=/data/Objects365/data
```

3. Create a New Directory to Store Images from the Validation Set:
```shell
mkdir -p ${BASE_DIR}/train/images_from_val
```

3. Copy the v1 and v2 folders from the val directory into the train/images_from_val directory
```shell
cp -r ${BASE_DIR}/val/images/v1 ${BASE_DIR}/train/images_from_val/
cp -r ${BASE_DIR}/val/images/v2 ${BASE_DIR}/train/images_from_val/
```

4. Directory structure after copying:

```shell
${BASE_DIR}/train
â”œâ”€â”€ images_from_val
â”œâ”€â”€ images
â”‚   â”œâ”€â”€ v1
â”‚   â”‚   â”œâ”€â”€ patch0
â”‚   â”‚   â”‚   â”œâ”€â”€ 000000000.jpg
â”‚   â”‚   â”‚   â”œâ”€â”€ 000000001.jpg
â”‚   â”‚   â”‚   â””â”€â”€ ... (more images)
â”‚   â”œâ”€â”€ v2
â”‚   â”‚   â”œâ”€â”€ patchx
â”‚   â”‚   â”‚   â”œâ”€â”€ 000000000.jpg
â”‚   â”‚   â”‚   â”œâ”€â”€ 000000001.jpg
â”‚   â”‚   â”‚   â””â”€â”€ ... (more images)
â”œâ”€â”€ zhiyuan_objv2_train.json
```

```shell
${BASE_DIR}/val
â”œâ”€â”€ images
â”‚   â”œâ”€â”€ v1
â”‚   â”‚   â”œâ”€â”€ patch0
â”‚   â”‚   â”‚   â”œâ”€â”€ 000000000.jpg
â”‚   â”‚   â”‚   â””â”€â”€ ... (more images)
â”‚   â”œâ”€â”€ v2
â”‚   â”‚   â”œâ”€â”€ patchx
â”‚   â”‚   â”‚   â”œâ”€â”€ 000000000.jpg
â”‚   â”‚   â”‚   â””â”€â”€ ... (more images)
â”œâ”€â”€ zhiyuan_objv2_val.json
```

5. Run remap_obj365.py to merge a subset of the validation set into the training set. Specifically, this script moves samples with indices between 5000 and 800000 from the validation set to the training set.
```shell
python tools/remap_obj365.py --base_dir ${BASE_DIR}
```


6. Run the resize_obj365.py script to resize any images in the dataset where the maximum edge length exceeds 640 pixels. Use the updated JSON file generated in Step 2 to process the sample data. Ensure that you resize images in both the train and val datasets to maintain consistency.
```shell
python tools/resize_obj365.py --base_dir ${BASE_DIR}
```

7. Modify paths in [obj365_detection.yml](./configs/dataset/obj365_detection.yml)

    ```yaml
    train_dataloader: 
        img_folder: /data/Objects365/data/train
        ann_file: /data/Objects365/data/train/new_zhiyuan_objv2_train_resized.json
    val_dataloader:
        img_folder:  /data/Objects365/data/val/
        ann_file:  /data/Objects365/data/val/new_zhiyuan_objv2_val_resized.json
    ```


</details>

<details>
<summary>Custom dataset</summary>

To train on your custom dataset, you need to organize it in the COCO format. Follow the steps below to prepare your dataset:

1. **Set `remap_mscoco_category` to `False`:**

    This prevents the automatic remapping of category IDs to match the MSCOCO categories.

    ```yaml
    remap_mscoco_category: False
    ```

2. **Organize Images:**

    Structure your dataset directories as follows:

    ```shell
    dataset/
    â”œâ”€â”€ images/
    â”‚   â”œâ”€â”€ train/
    â”‚   â”‚   â”œâ”€â”€ image1.jpg
    â”‚   â”‚   â”œâ”€â”€ image2.jpg
    â”‚   â”‚   â””â”€â”€ ...
    â”‚   â”œâ”€â”€ val/
    â”‚   â”‚   â”œâ”€â”€ image1.jpg
    â”‚   â”‚   â”œâ”€â”€ image2.jpg
    â”‚   â”‚   â””â”€â”€ ...
    â””â”€â”€ annotations/
        â”œâ”€â”€ instances_train.json
        â”œâ”€â”€ instances_val.json
        â””â”€â”€ ...
    ```

    - **`images/train/`**: Contains all training images.
    - **`images/val/`**: Contains all validation images.
    - **`annotations/`**: Contains COCO-formatted annotation files.

3. **Convert Annotations to COCO Format:**

    If your annotations are not already in COCO format, you'll need to convert them. You can use the following Python script as a reference or utilize existing tools:

    ```python
    import json

    def convert_to_coco(input_annotations, output_annotations):
        # Implement conversion logic here
        pass

    if __name__ == "__main__":
        convert_to_coco('path/to/your_annotations.json', 'dataset/annotations/instances_train.json')
    ```

4. **Update Configuration Files:**

    Modify your [custom_detection.yml](./configs/dataset/custom_detection.yml).

    ```yaml
    task: detection
    
    evaluator:
      type: CocoEvaluator
      iou_types: ['bbox', ]

    num_classes: 777 # your dataset classes
    remap_mscoco_category: False
    
    train_dataloader: 
      type: DataLoader
      dataset: 
        type: CocoDetection
        img_folder: /data/yourdataset/train
        ann_file: /data/yourdataset/train/train.json
        return_masks: False
        transforms:
          type: Compose
          ops: ~
      shuffle: True
      num_workers: 4
      drop_last: True 
      collate_fn:
        type: BatchImageCollateFuncion
    
    val_dataloader:
      type: DataLoader
      dataset: 
        type: CocoDetection
        img_folder: /data/yourdataset/val
        ann_file: /data/yourdataset/val/ann.json
        return_masks: False
        transforms:
          type: Compose
          ops: ~ 
      shuffle: False
      num_workers: 4
      drop_last: False
      collate_fn:
        type: BatchImageCollateFuncion
    ```

</details>

## Usage
<details>
<summary> COCO2017 </summary>

<!-- <summary>1. Training </summary> -->
1. Set Model:
```shell
export model=l
```

2. Training
```shell
CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --use-amp --seed=0
```

<!-- <summary>2. Testing </summary> -->
3. Testing
```shell
CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth --test-only
```

<!-- <summary>3. Tuning </summary> -->
4. Tuning
```shell
CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -t model.pth --use-amp --seed=0
```
</details>


<details>
<summary> Objects to COCO </summary>

1. Set Model:
```shell
export model=l
```

2. Training on Objects365
```shell
CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=777 --nproc_per_node=4 train.py -c configs/dfine/objects365/dfine_hgnetv2_${model}_obj365.yml --use-amp --seed=0
```

3. Turning on COCO2017
```shell
CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=777 --nproc_per_node=4 train.py -c configs/dfine/objects365/dfine_hgnetv2_${model}_obj2coco.yml --use-amp --seed=0 -t model.pth
```

<!-- <summary>2. Testing </summary> -->
4. Testing
```shell
CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth --test-only
```
</details>


<details>
<summary> Custom dataset </summary>

1. Set Model:
```shell
export model=l
```

2. Training on Custom dataset
```shell
CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=777 --nproc_per_node=4 train.py -c configs/dfine/custom/dfine_hgnetv2_${model}_custom.yml --use-amp --seed=0
```
<!-- <summary>2. Testing </summary> -->
3. Testing
```shell
CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=777 --nproc_per_node=4 train.py -c configs/dfine/custom/dfine_hgnetv2_${model}_custom.yml -r model.pth --test-only
```
</details>


<details>
<summary> Deployment </summary>

<!-- <summary>4. Export onnx </summary> -->
1. Setup:
```shell
export model=l
pip install onnx onnxsim
```

2. Export onnx and tensorrt
```shell
python tools/export_onnx.py --check -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth
```

3. Export [tensorrt](https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html)
```shell
trtexec --onnx="model.onnx" --saveEngine="model.engine" --fp16
```

</details>

<details>
<summary> Inference / Benchmark / Visualization </summary>


1. Setup:
```shell
export model=l
pip install -r benchmark/requirements.txt
```


<!-- <summary>5. Inference </summary> -->
2. Inference (onnxruntime / tensorrt / torch)
```shell
python benchmark/inference/onnx_inf.py --onnx-file model.onnx --im-file image.jpg
python benchmark/inference/trt_inf.py --trt-file model.trt --im-file image.jpg
python benchmark/inference/torch_inf.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth --im-file image.jpg --device cuda:0
```

<!-- <summary>6. Benchmark </summary> -->
3. Benchmark (Params. / GFLOPs / Latency)
```shell
python benchmark/get_info.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml
python benchmark/trt_benchmark.py --COCO_dir path/to/COCO2017 --engine_dir model.engine
```

4. Voxel51 Fiftyone Visualization ([fiftyone](https://github.com/voxel51/fiftyone))
```shell
pip install fiftyone
python tools/fiftyone.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth
```
</details>



## Citation
If you use `D-FINE` in your work, please use the following BibTeX entries:

<summary> bibtex </summary>

```latex

```
</details>

## Acknowledgement
Our work is built upon [RT-DETR](https://github.com/lyuwenyu/RT-DETR).
Thanks to the inspirations from [RT-DETR](https://github.com/lyuwenyu/RT-DETR), [GFocal](https://github.com/implus/GFocal), [LD](https://github.com/HikariTJU/LD), and [YOLOv9](https://github.com/WongKinYiu/yolov9).

âœ¨ Feel free to contribute and reach out if you have any questions! âœ¨